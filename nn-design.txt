= Neural net architecture =

There are some hyperparameters in this design:

1. Kernel size, k. k=3 seems sensible
2. Filter count, f. f=256 is what the alphago paper chose
3. Residual layer count, r. the alphago paper considered both r=19 and r=39;
   given our low compute power we'll go with r=19 for now
4. Leakiness, l. I picked l=0.1 because it seemed like a good idea at the time
5. Regularization constant, c. I picked c=0.0001 because cargo cult lol

== Input ==

An 18x8x16 array of 0s and 1s. I'll use "channel" for the first dimension, "x"
or "column" for the second, and "y" or "row" for the third.

The Dr. Mario board is 8x16. Following the maryodel protocol, the bottom left
of the board is at the lowest x- and y-coordinates, so (0,0), and the top right
is at (7,15).

The first 3 channels encode the positions of viruses:

00. 1s where red viruses are, 0s elsewhere
01. yellow viruses
02. blue viruses

The next 15 channels also come in groups of three (for red, yellow, blue in
that order), and encode the positions of:

03-05. disconnected pill halves (top- and bottom-pill halves are considered
       disconnected for this purpose)
06-08. left pill halves
09-11. right pill halves
12-14. a channel that is 1s everywhere if the left  half of the next pill is
       the appropriate color, 0s everywhere if not
15-17. a channel that is 1s everywhere if the right half of the next pill is
       the appropriate color, 0s everywhere if not

== Layers ==

* A 2D convolutional layer with kernel size k, filter count f, zero-padding,
  and stride 1
* Batch normalization
* A leaky ReLU nonlinearity with parameter l
* A bunch (r) of residual layers; each residual layer is:
    * A 2D convolutional layer with kernel size k, filter count f,
      zero-padding, and stride 1
    * Batch normalization
    * A leaky ReLU nonlinearity with parameter l
    * A 2D convolutional layer with kernel size k, filter count f,
      zero-padding, and stride 1
    * Batch normalization
    * Skip connection
    * A leaky ReLU nonlinearity with parameter l
* A fully-connected layer to 1+1+1+4x8x16 pre-outputs
* In parallel:
    * An single scalar tanh to produce a "won" indicator
    * A single scalar softplus to produce a "cleared" virus count, with beta=1,
      threshold=5
    * A single scalar softplus to produce a "duration", how many pills we
      placed, with beta=1, threshold=5
    * A 4x8x16 inverse-logit + rescaling to produce a "moves" probability
      distribution

== Output ==

The output has four parts:
* A single number in [-1,1]; -1 indicates that the game ended by topping out, 1
  that the game ended either in stalemate or by clearing all the viruses.
* A single non-negative number, indicating how many viruses were cleared by the
  time the game ended.
* A single non-negative number, indicating how many pills were used by the time
  the game ended.
* A 4x8x16 array of numbers in [0,1]. The first dimension gives the rotation of
  the original pill, in number of clockwise rotations mod 4. The second and
  third dimensions are position, with the same convention as for the input:
  (0,0) is bottom left and (7,15) is top right. The number tells the
  probability of choosing that move. (For training examples, this is
  traditionally computed as the fraction of MCTS expansions that passed through
  that move, but in our round-0 training with no neural net, it's proportional
  to the computed value of the move instead). No distinction is made in the
  spec between illegal moves and moves that the engine will never pick -- 0
  could mean either thing -- but it should be very rare indeed that legal moves
  have probability 0 in the training examples produced by the implementation
  used here.

  The numbers in this array sum to 1 (up to rounding). In training examples, if
  the pill has both colors the same, the 8x16 subarrays at indices 0 and 2 will
  be the same, and the 8x16 subarrays at indices 1 and 3 will be the same.

== Loss ==

We use cross-entropy for the probability-like outputs, L2 norm for the win
prediction, a parameter regularization term, and then a somewhat non-standard
scaled L2 norm for the rest:

      (won_predicted - won_ground_truth)^2
    + ((cleared_predicted - cleared_ground_truth) / cleared_ground_truth)^2
    + ((duration_predicted - duration_ground_truth) / duration_ground_truth)^2
    - moves_ground_truth * log(moves_predicted)
    + c||parameters||^2

What's up with the non-standard norm? I found that using a straight L2-norm
made choosing a learning rate very difficult. Because the numbers for cleared
and duration are much larger than the others -- often in the range of 50-100 --
their contribution to the loss is large, and the gradient of the loss is
correspondingly large. This meant that increasing batch size, or even switching
from one training batch to another, wildly changed what the appropriate choice
of learning rate was. By scaling in this way, the numbers stay closer to 1-ish
when the net is doing well (and closer to 0-ish at the start of training),
meaning these terms' contribution to the loss function is on par with the other
terms.

== JSON representation ==

A training example can be stored in a JSON array of this shape:

    [board, pill, won, cleared, duration, moves]

The elements of this array are described below.

* board: A string, in the maryodel board format, describing the board.
* pill: A string, in the maryodel pill format, describing the next pill. The
  pill must be horizontal.
* won: Either -1 or 1, the first neural net output.
* cleared: A non-negative integer, the second neural net output.
* duration: A non-negative integer, the third neural net output.
* moves: An array of length 4. The index matches the first dimension of the
  neural net output. Each element is itself an array, of indeterminate (and not
  necessarily matching) lengths. The length must be a multiple of 3. Each chunk
  of 3 is an x,y,p triple giving an x coordinate, a y coordinate, and a
  probability of choosing to put the pill at that position in the current
  rotation amount. Together these are a sparse encoding of the fourth neural
  net output.

  For example, consider this array:

      [[3,5,0.5]
      ,[6,7,0.2,2,4,0.3]
      ,[]
      ,[]
      ]

  This indicates that the MCTS search favored putting the current pill at (3,5)
  without rotating it half the time, at (6,7) with one rotation 0.2 of the
  time, and at (2,4) with one rotation 0.3 of the time.
